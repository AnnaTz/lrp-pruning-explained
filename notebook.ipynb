{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<strong>Pruning by Explaining</strong><br>\n",
    "A Novel Criterion for Deep Neural Network Pruning<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNNs: Applications & Drawbacks**\n",
    "\n",
    "Deep CNNs have achieved <u>high predictive performance</u> in a <u>wide range of applications</u>.\n",
    "\n",
    "However, two major <u>drawbacks</u>:\n",
    "1. Deep CNNs have <u>high requirements</u> in:\n",
    "   - storage\n",
    "   - computational costs\n",
    "   - energy expenditure\n",
    "   - runtime\n",
    "   - memory\n",
    "2. The automatic, non-deterministic learning of features makes their understanding and <u>comprehension difficult</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network pruning**\n",
    "\n",
    "- Neural network pruning aims to reduce requirements via <u>eliminating the least important network unit</u> w.r.t. the network’s intended task.\n",
    "\n",
    "- A <u>criterion</u> to identify the “irrelevant” subset of the parameters meant for deletion <u>is required</u>.\n",
    "\n",
    "- Examples of proposed criteria :\n",
    "  - <u>Taylor expansion</u>: [1] used a first-order Taylor expansion to approximate the change of loss in the objective function as an effect of pruning away network units.\n",
    "  - <u>Gradient</u>: [2] introduced a sparsified back-propagation training approach using the magnitude of the gradient to find essential and non-essential features.\n",
    "  - <u>Weight</u>: [3] proposed a one-shot channel pruning method using the norm of weights for filter selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explainability**\n",
    "\n",
    "<u>Explainability</u> aims to <u>make an opaque model understandable</u> via answering questions like:\n",
    "- Which parts of the input influenced an inference decision?\n",
    "- How should the input change in order to lead to another inference decision?\n",
    "- What concepts in the input influenced an inference decision?\n",
    "- Which parameters of the network influenced an inference decision?\n",
    "- Which training data points influenced an inference decision?\n",
    "- How would a human describe an inference decision?\n",
    "- What knowledge does each parameter of the network contain?\n",
    "- What principles define how the training algorithm distributes knowledge in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer-wise Relevance Propagation**\n",
    "\n",
    "[4] proposed a <u>novel criterion for pruning</u> neural network units based on an explainability method: <u>Layer-wise Relevance Propagation</u> (LRP) [5].\n",
    "- LRP decomposes a classification decision into proportionate *relevances* of each network unit\n",
    "- based on a backward pass that redistributes the network’s output to all its units layer-by-layer.\n",
    "- The relevance scores reflect the contribution of each unit to the network’s information flow.\n",
    "- LRP is an easily scalable method - not dependent on layer types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LRP-based pruning overview**\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/image_1.png\" alt=\"Alt text\">\n",
    "  <figcaption><u>Figure 1</u>: Illustration of LRP-based sequential process for pruning.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LRP properties**\n",
    "\n",
    "- LRP <u>layer-wise conservation property</u>: $\\sum_{i}{R_{i \\leftarrow j}^{(l)}} = R_{j}^{(l+1)}$\n",
    "  - where $R_{j}^{(l+1)}$ is the relevance of neuron j at the layer l+1,\n",
    "  - $R_{i \\leftarrow j}^{(0)}$ is the share of $R_{j}^{(l+1)}$ that is redistributed to neuron i in the lower layer,\n",
    "  - and the sum runs over all neurons i of the preceding layer l.\n",
    "  \n",
    "- At each layer l: the node i's global importance can be extracted as its attributed relevance $R_{i}^{(l)}$.\n",
    "  \n",
    "- The propagated prediction relevance is preserved between neurons of two adjacent layers.\n",
    "  \n",
    "- When using relevance as a pruning criterion:\n",
    "  - <u>intrinsic normalization layer-by-layer</u>, regardless of hidden layer size and the number of iteratively pruned neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LRP propagation**\n",
    "\n",
    "- Relevancies get propagated across the network’s neurons following the LRP - $a_1b_0$ rule:\n",
    "  - developed for feedforward-DNNs with ReLU activations\n",
    "  - assumes positive (pre-softmax) logit activations\n",
    "\n",
    "- Propagation rule: $R_{i}^{(l)} = \\sum_{j}{\\frac{(a_{i}^{(l)}w_{ij})^{+}}{\\sum_{k}{(a_{k}^{(l)}w_{kj})^{+}}}R_{j}^{(l+1)}}$\n",
    "  - where $R_{i}^{(l)}$ denotes relevance attributed to the i neuron at layer l,\n",
    "  - as an aggregation of downward-propagated relevance messages $R_{i \\leftarrow j}^{(0)}$,\n",
    "  - $(\\ )^{+}$ indicate the positive part of the forward propagated quantities from layer l to layer l+1,\n",
    "  - and k is a running index over all input activations a.\n",
    "\n",
    "- The algorithm is initialized with $R_{c}^{(L)} = 1$ at the output layer L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LRP-based pruning advantages**\n",
    "\n",
    "Thanks to its <u>locally conservative propagation rule</u>, LRP-based pruning has the following technical <u>advantages over other pruning techniques</u> such as gradient-based or activation-based methods:\n",
    "\n",
    "- layer-wise <u>regularized global redistribution</u> of importances from each network unit.\n",
    "- directly applicable as a measure of total relevance per node or filter (by summing relevancies), <u>without requiring a post-hoc layer-wise renormalization</u> (e.g., $l_p$ norm).\n",
    "- <u>not restricted to a global application</u> of pruning but easily applicable to locally and (neuron- or filter-)group-wise constrained pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/image_2.png\" alt=\"Alt text\">\n",
    "  <figcaption><u>Figure 2</u>: Comparison of test accuracy in different criteria as pruning rate increases on VGG-16 and ResNet-50 with five datasets.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/image_3.png\" alt=\"Alt text\">\n",
    "  <figcaption><u>Figure 3</u>: An observation of per-layer pruning performed w.r.t the different evaluated criteria on VGG-16 and two datasets. <br> Each colored line corresponds to a specific (global) ratio of filters pruned from the network (black (top): 0%, red: 15%, green: 30%, blue: 45%, violet: 75% and black (bottom): 90%).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/image_4.png\" alt=\"Alt text\">\n",
    "  <figcaption><u>Table 1</u>: A performance comparison between criteria (Weight, Taylor, Gradient with l2-norm each and LRP) and the Unpruned model for VGG-16 (top) and ResNet-50 (bottom) on five different image benchmark datasets. <br> Accuracy (in %), Loss (×10<sup>−2</sup>), Params (×10<sup>7</sup>) and FLOPs (in GMAC) per forward pass.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/image_5.png\" alt=\"Alt text\">\n",
    "  <figcaption><u>Figure 4</u>: Test accuracy after pruning of n% of convolutional and m% of fully connected filters on VGG-16 without fine-tuning for a random subset of the classes from ILSVRC 2012 (k = 3) based on different criteria (averaged over 20 repetitions).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/image_6.png\" alt=\"Alt text\">\n",
    "  <figcaption><u>Figure 5</u>: Test accuracy after pruning of n% of convolutional filters on ResNet18 and ResNet50 without fine-tuning for a random subset of the classes from ILSVRC 2012 (k = 3) based on the criteria Weight, Taylor, Gradient with l2- norm and LRP (averaged over 20 repetitions).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/image_7.png\" alt=\"Alt text\">\n",
    "  <figcaption><u>Figure 6</u>: Performance comparison of pruning without fine-tuning for AlexNet, VGG-16, ResNet-18 and ResNet-50 based on only few (10) samples per class from the Cats & Dogs dataset, as a means for domain adaption.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<u>Benefits</u> of LRP-based pruning:\n",
    "- <u>competitive or better efficiency</u> compared to state-of-the-art pruning criteria when successive retraining is performed\n",
    "- clearly outperforming these previous criteria in the <u>resource-constrained application</u> scenario\n",
    "- <u>low computational cost</u> in the order of gradient computation\n",
    "- comparatively <u>simple to apply</u> without the need for tuning hyperparameters for pruning\n",
    "- <u>naturally normalized</u>, thus explicit regularization via normalization is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[1] P. Molchanov, S. Tyree, T. Karras, T. Aila, J. Kautz, Pruning convolutional neural networks for resource efficient transfer learning, in: Proceedings of the International Conference on Learning Representations (ICLR), 2017.\n",
    "\n",
    "[2] X. Sun, X. Ren, S. Ma, H. Wang, meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting, in: International Conference on Machine Learning (ICML), 2017, pp. 3299–3308.\n",
    "\n",
    "[3] H. Li, A. Kadav, I. Durdanovic, H. Samet, H. P. Graf, Pruning filters for efficient convnets, in: International Conference on Learning Representations, (ICLR), 2017.\n",
    "\n",
    "[4] Yeom, Seul-Ki & Seegerer, Philipp & Lapuschkin, Sebastian & Binder, Alexander & Wiedemann, Simon & Müller, Klaus-Robert & Samek, Wojciech. (2021). Pruning by Explaining: A Novel Criterion for Deep Neural Network Pruning. Pattern Recognition. 115. 107899. 10.1016/j.patcog.2021.107899.\n",
    "\n",
    "[5] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, W. Samek, On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation, PLoS ONE 10 (2015) e0130140."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
